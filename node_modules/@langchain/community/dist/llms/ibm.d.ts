import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { BaseLLM, BaseLLMParams } from "@langchain/core/language_models/llms";
import { DeploymentsTextGenerationParams, DeploymentsTextGenerationStreamParams, DeploymentTextGenProperties, ReturnOptionProperties, TextGenerationParams, TextGenerationStreamParams, TextGenLengthPenalty, TextGenParameters, TextTokenizeParameters } from "@ibm-cloud/watsonx-ai/dist/watsonx-ai-ml/vml_v1.js";
import { LLMResult, GenerationChunk } from "@langchain/core/outputs";
import { BaseLanguageModelCallOptions } from "@langchain/core/language_models/base";
import { WatsonxAuth, WatsonxParams } from "../types/ibm.js";
/**
 * Input to LLM class.
 */
export interface WatsonxCallOptionsLLM extends BaseLanguageModelCallOptions, Omit<Partial<TextGenerationParams & TextGenerationStreamParams & DeploymentsTextGenerationParams & DeploymentsTextGenerationStreamParams>, "input"> {
    maxRetries?: number;
}
export interface WatsonxInputLLM extends TextGenParameters, WatsonxParams, BaseLLMParams {
    streaming?: boolean;
}
/**
 * Integration with an LLM.
 */
export declare class WatsonxLLM<CallOptions extends WatsonxCallOptionsLLM = WatsonxCallOptionsLLM> extends BaseLLM<CallOptions> implements WatsonxInputLLM {
    static lc_name(): string;
    lc_serializable: boolean;
    streaming: boolean;
    model: string;
    maxRetries: number;
    version: string;
    serviceUrl: string;
    max_new_tokens?: number;
    spaceId?: string;
    projectId?: string;
    idOrName?: string;
    decoding_method?: TextGenParameters.Constants.DecodingMethod | string;
    length_penalty?: TextGenLengthPenalty;
    min_new_tokens?: number;
    random_seed?: number;
    stop_sequences?: string[];
    temperature?: number;
    time_limit?: number;
    top_k?: number;
    top_p?: number;
    repetition_penalty?: number;
    truncate_input_tokens?: number;
    return_options?: ReturnOptionProperties;
    include_stop_sequence?: boolean;
    maxConcurrency?: number;
    private service;
    constructor(fields: WatsonxInputLLM & WatsonxAuth);
    get lc_secrets(): {
        [key: string]: string;
    };
    get lc_aliases(): {
        [key: string]: string;
    };
    invocationParams(options: this["ParsedCallOptions"]): TextGenParameters | DeploymentTextGenProperties;
    scopeId(): {
        projectId: string;
        modelId: string;
        idOrName?: undefined;
        spaceId?: undefined;
    } | {
        idOrName: string;
        modelId: string;
        projectId?: undefined;
        spaceId?: undefined;
    } | {
        spaceId: string | undefined;
        modelId: string;
        projectId?: undefined;
        idOrName?: undefined;
    };
    listModels(): Promise<string[] | undefined>;
    private generateSingleMessage;
    completionWithRetry<T>(callback: () => T, options?: this["ParsedCallOptions"]): Promise<T>;
    _generate(prompts: string[], options: this["ParsedCallOptions"], _runManager?: CallbackManagerForLLMRun): Promise<LLMResult>;
    getNumTokens(content: string, options?: TextTokenizeParameters): Promise<number>;
    _streamResponseChunks(prompt: string, options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<GenerationChunk>;
    _llmType(): string;
}
